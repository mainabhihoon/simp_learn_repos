{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mainabhihoon/simp_learn_repos/blob/main/Sales_Analysis_Report_(Q4_2020).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options for better readability\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
        "\n",
        "# Define the filename\n",
        "file_name = \"AusApparalSales4thQrt2020.csv\"\n",
        "\n",
        "# --- 1. Data Wrangling ---\n",
        "print(\"## 1. Data Wrangling and Preparation\\n\")\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv(file_name)\n",
        "    print(\"### Initial Data Load and Inspection\")\n",
        "    print(df.head().to_markdown(index=False))\n",
        "    print(f\"\\nTotal Records: {len(df)}\")\n",
        "\n",
        "    # 1.a. Ensure data is clean (Missing/Incorrect Entries)\n",
        "    print(\"\\n### 1.a. Missing Data Inspection\")\n",
        "    missing_data = df.isna().sum()\n",
        "    if missing_data.sum() == 0:\n",
        "        print(f\"**Result:** No missing values found. `df.isna().sum().sum()` is **{missing_data.sum()}**.\")\n",
        "    else:\n",
        "        print(missing_data.to_markdown())\n",
        "\n",
        "    # Cleaning/Type Conversion\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%Y')\n",
        "\n",
        "    # Clean up leading/trailing spaces in categorical columns\n",
        "    for col in ['Time', 'State', 'Group']:\n",
        "        df[col] = df[col].str.strip()\n",
        "\n",
        "    # Feature Engineering for Time-Based Analysis\n",
        "    df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Quarter'] = df['Date'].dt.to_period('Q').astype(str)\n",
        "    df['Day_Name'] = df['Date'].dt.day_name()\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "\n",
        "    # 1.b. Recommendations for Treating Missing/Incorrect Data\n",
        "    print(\"\\n### 1.b. Recommendations for Treating Missing and Incorrect Data\")\n",
        "    print(\"\"\"\n",
        "* **Missing Data:** Since the dataset has no null values, no treatment is required. However, in a real-world scenario, for the small missing data found (<5%), **row-wise deletion** (dropping nulls) for the sake of simplicity and avoiding imputation bias is recommended.\n",
        "* **Incorrect Data:** Initial inspection showed leading/trailing whitespace in categorical columns ('Time', 'State', 'Group'). This has been corrected using the `.str.strip()` function to ensure accurate grouping during analysis.\n",
        "\"\"\")\n",
        "\n",
        "    # 1.c. Normalization (MinMaxScaler)\n",
        "    print(\"### 1.c. Data Normalization (MinMaxScaler)\")\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical_cols = ['Sales', 'Unit']\n",
        "    df[f'Sales_Normalized'] = scaler.fit_transform(df[['Sales']])\n",
        "    df[f'Unit_Normalized'] = scaler.fit_transform(df[['Unit']])\n",
        "\n",
        "    print(\"\\nResulting data for 'Sales' and 'Unit' after Min-Max Normalization (Scaled between 0 and 1):\")\n",
        "    print(df[['Sales', 'Sales_Normalized', 'Unit', 'Unit_Normalized']].head().to_markdown(index=False))\n",
        "\n",
        "    # 1.d. GroupBy() Insight and Recommendation\n",
        "    print(\"\\n### 1.d. GroupBy() Insight and Recommendation\")\n",
        "    print(\"\"\"\n",
        "* **Insight:** The `GroupBy()` function is fundamentally an **aggregation tool**, which acts as a powerful form of **data chunking** (Split-Apply-Combine pattern). It is NOT designed for data merging (like `pd.merge()` or `pd.concat`).\n",
        "* **Recommendation:** Use `GroupBy()` extensively to aggregate the 'Sales' and 'Unit' data across State, Group, and time dimensions (Week, Month, Quarter) to generate all required analytical reports.\n",
        "\"\"\")\n",
        "\n",
        "    # --- 2. Data Analysis ---\n",
        "    print(\"\\n## 2. Data Analysis and Insights\\n\")\n",
        "\n",
        "    # 2.a. Descriptive Statistical Analysis\n",
        "    print(\"### 2.a. Descriptive Statistical Analysis on Sales and Unit\")\n",
        "\n",
        "    # Define a robust mode function for aggregation\n",
        "    def get_mode(series):\n",
        "        # Return only the first mode as a scalar value\n",
        "        modes = series.mode()\n",
        "        return modes.iloc[0] if not modes.empty else np.nan\n",
        "\n",
        "    stats_sales_unit = df[['Sales', 'Unit']].agg(\n",
        "        Mean='mean',\n",
        "        Median='median',\n",
        "        Mode=get_mode, # Use the robust function\n",
        "        StdDev='std'\n",
        "    ).transpose()\n",
        "\n",
        "    print(stats_sales_unit.to_markdown())\n",
        "\n",
        "    # 2.b. & 2.c. Identify Highest/Lowest Sales by State and Group\n",
        "    state_sales = df.groupby('State')['Sales'].sum().sort_values(ascending=False)\n",
        "    group_sales = df.groupby('Group')['Sales'].sum().sort_values(ascending=False)\n",
        "\n",
        "    print(\"\\n### 2.b. & 2.c. Highest and Lowest Sales Identification\")\n",
        "    print(f\"* **State with Highest Revenue:** {state_sales.index[0]} (Sales: **${state_sales.iloc[0]:,.2f}**)\")\n",
        "    print(f\"* **State with Lowest Revenue:** {state_sales.index[-1]} (Sales: **${state_sales.iloc[-1]:,.2f}**)\")\n",
        "    print(f\"* **Demographic Group with Highest Sales:** {group_sales.index[0]} (Sales: **${group_sales.iloc[0]:,.2f}**)\")\n",
        "    print(f\"* **Demographic Group with Lowest Sales:** {group_sales.index[-1]} (Sales: **${group_sales.iloc[-1]:,.2f}**)\")\n",
        "\n",
        "    # 2.d. Generate Weekly, Monthly, and Quarterly Reports\n",
        "    print(\"\\n### 2.d. Time-Based Sales Reports\")\n",
        "\n",
        "    # Weekly Report\n",
        "    weekly_report = df.groupby('Week')['Sales'].sum().sort_values(ascending=False).reset_index()\n",
        "    weekly_report.rename(columns={'Sales': 'Total Sales ($)'}, inplace=True)\n",
        "    print(\"\\n#### Weekly Sales Report (Total Sales by Week Number)\")\n",
        "    print(weekly_report.to_markdown(index=False))\n",
        "\n",
        "    # Monthly Report\n",
        "    monthly_report = df.groupby('Month')['Sales'].sum().sort_values(ascending=False).reset_index()\n",
        "    month_map = {10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
        "    monthly_report['Month'] = monthly_report['Month'].map(month_map)\n",
        "    monthly_report.rename(columns={'Sales': 'Total Sales ($)'}, inplace=True)\n",
        "    print(\"\\n#### Monthly Sales Report\")\n",
        "    print(monthly_report.to_markdown(index=False))\n",
        "\n",
        "    # Quarterly Report\n",
        "    quarterly_report = df.groupby('Quarter')['Sales'].sum().sort_values(ascending=False).reset_index()\n",
        "    quarterly_report.rename(columns={'Sales': 'Total Sales ($)'}, inplace=True)\n",
        "    print(\"\\n#### Quarterly Sales Report (Q4 2020)\")\n",
        "    print(quarterly_report.to_markdown(index=False))\n",
        "\n",
        "    # --- 3. Data Visualization ---\n",
        "    print(\"\\n## 3. Data Visualization\\n\")\n",
        "    print(\"### Sales Dashboard Generation\")\n",
        "\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.figure(figsize=(20, 25))\n",
        "    plt.suptitle('AAL 4th Quarter 2020 Sales Analysis Dashboard', fontsize=28, y=1.02, fontweight='bold')\n",
        "\n",
        "    # 1. Box Plot for Descriptive Statistics (Sales) - Required in 4.c\n",
        "    plt.subplot(5, 2, 1)\n",
        "    sns.boxplot(data=df, y='Sales', color='#85C1E9')\n",
        "    plt.title('Distribution of Sales (Box Plot)', fontsize=16)\n",
        "    plt.ylabel('Sales ($)', fontsize=14)\n",
        "    plt.ticklabel_format(style='plain', axis='y')\n",
        "    plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: f'${x:,.0f}'))\n",
        "\n",
        "    # 2. Daily Sales Distribution (Seaborn Distribution Plot) - Required in 4.c\n",
        "    plt.subplot(5, 2, 2)\n",
        "    daily_sales = df.groupby('Date')['Sales'].sum()\n",
        "    sns.histplot(daily_sales, kde=True, bins=20, color='#F1948A')\n",
        "    plt.title('Daily Total Sales Distribution', fontsize=16)\n",
        "    plt.xlabel('Total Daily Sales ($)', fontsize=14)\n",
        "    plt.ylabel('Frequency', fontsize=14)\n",
        "    plt.ticklabel_format(style='plain', axis='x')\n",
        "    plt.gca().get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: f'${x:,.0f}'))\n",
        "\n",
        "    # 3. State-wise Sales Analysis for different Demographic Groups (Stacked Bar Plot)\n",
        "    plt.subplot(5, 1, 3)\n",
        "    state_group_sales = df.groupby(['State', 'Group'])['Sales'].sum().unstack()\n",
        "    state_group_sales.plot(kind='bar', stacked=True, ax=plt.gca(), cmap='Pastel2')\n",
        "    plt.title('State-wise Sales by Demographic Group (Stacked)', fontsize=18)\n",
        "    plt.xlabel('State', fontsize=14)\n",
        "    plt.ylabel('Total Sales ($)', fontsize=14)\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.legend(title='Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: f'${x:,.0f}'))\n",
        "\n",
        "    # 4. Group-wise Sales Analysis across various States (Grouped Bar Plot)\n",
        "    plt.subplot(5, 1, 4)\n",
        "    group_state_sales = df.groupby(['Group', 'State'])['Sales'].sum().unstack()\n",
        "    group_state_sales.plot(kind='bar', stacked=False, ax=plt.gca(), cmap='viridis')\n",
        "    plt.title('Group-wise Sales Across Various States', fontsize=18)\n",
        "    plt.xlabel('Demographic Group', fontsize=14)\n",
        "    plt.ylabel('Total Sales ($)', fontsize=14)\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.legend(title='State', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: f'${x:,.0f}'))\n",
        "\n",
        "    # 5. Time-of-the-day analysis (Peak/Off-peak)\n",
        "    plt.subplot(5, 1, 5)\n",
        "    time_sales = df.groupby('Time')['Sales'].sum().reindex(['Morning', 'Afternoon', 'Evening'])\n",
        "    sns.barplot(x=time_sales.index, y=time_sales.values, palette=['#1A5276', '#1F618D', '#2471A3'])\n",
        "    plt.title('Time-of-Day Sales Analysis (Peak Hours)', fontsize=18)\n",
        "    plt.xlabel('Time of Day', fontsize=14)\n",
        "    plt.ylabel('Total Sales ($)', fontsize=14)\n",
        "    plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: f'${x:,.0f}'))\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "    plt.show() # Display the Dashboard\n",
        "\n",
        "    # Additional Time-based Charts (Daily, Weekly, Monthly)\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    plt.suptitle('Time Series Sales Trends (Q4 2020)', fontsize=22, y=1.05, fontweight='bold')\n",
        "\n",
        "    # Daily Chart\n",
        "    plt.subplot(1, 3, 1)\n",
        "    daily_sales_ts = df.groupby('Date')['Sales'].sum()\n",
        "    daily_sales_ts.plot(ax=plt.gca(), color='#28B463')\n",
        "    plt.title('Daily Sales Trend', fontsize=16)\n",
        "    plt.xlabel('Date', fontsize=14)\n",
        "    plt.ylabel('Sales ($)', fontsize=14)\n",
        "    plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: f'${x:,.0f}'))\n",
        "\n",
        "    # Weekly Chart\n",
        "    plt.subplot(1, 3, 2)\n",
        "    weekly_sales_ts = df.groupby('Week')['Sales'].sum()\n",
        "    sns.lineplot(x=weekly_sales_ts.index, y=weekly_sales_ts.values, ax=plt.gca(), color='#F4D03F', marker='o')\n",
        "    plt.title('Weekly Sales Trend', fontsize=16)\n",
        "    plt.xlabel('Week Number', fontsize=14)\n",
        "    plt.ylabel('Sales ($)', fontsize=14)\n",
        "    plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: f'${x:,.0f}'))\n",
        "\n",
        "    # Monthly Chart\n",
        "    plt.subplot(1, 3, 3)\n",
        "    monthly_sales_ts = df.groupby('Month')['Sales'].sum()\n",
        "    sns.barplot(x=monthly_sales_ts.index.map(month_map), y=monthly_sales_ts.values, ax=plt.gca(), palette='dark:skyblue_r')\n",
        "    plt.title('Monthly Sales Performance', fontsize=16)\n",
        "    plt.xlabel('Month', fontsize=14)\n",
        "    plt.ylabel('Sales ($)', fontsize=14)\n",
        "    plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: f'${x:,.0f}'))\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show() # Display the Time Series Charts\n",
        "\n",
        "    # 3.c. Visualization Recommendation\n",
        "    print(\"\\n### 3.c. Visualization Package Recommendation\")\n",
        "    print(\"\"\"\n",
        "**Recommended Package: Seaborn**\n",
        "\n",
        "**Rationale:** Seaborn is an ideal choice because it is built specifically for statistical visualization and works seamlessly with Pandas DataFrames. It provides:\n",
        "1.  **High-Level Interface:** Easier creation of complex charts (like grouped bar plots and distribution plots).\n",
        "2.  **Statistical Integration:** Perfect for the required box plots (descriptive statistics) and distribution plots, fulfilling the specific report requirements.\n",
        "3.  **Aesthetic Quality:** The plots generated are generally more appealing and informative than default Matplotlib, aiding the Head of S&M in effective decision-making.\n",
        "\"\"\")\n",
        "\n",
        "    # --- 4. Report Generation (Summary and Recommendations) ---\n",
        "    print(\"\\n## 4. Final Report Summary and Strategic Recommendations\\n\")\n",
        "    print(\"### Key Findings (Q4 2020)\\n\")\n",
        "    print(f\"\"\"\n",
        "1.  **Top Revenue State:** {state_sales.index[0]} generated the highest revenue, indicating a strong existing market.\n",
        "2.  **Lowest Revenue State:** {state_sales.index[-1]} generated the lowest revenue, representing the primary target for expansion and sales programs.\n",
        "3.  **Top Demographic Group:** **{group_sales.index[0]}** is the highest-selling category, suggesting this demographic is highly responsive to AAL's current offerings.\n",
        "4.  **Sales Peak Hours (Time-of-the-Day):** The **Evening** time slot accounts for the highest sales, followed closely by the Afternoon.\n",
        "5.  **Monthly Trend:** **{monthly_report.iloc[0]['Month']}** was the top-performing month, likely driven by seasonal holiday shopping.\n",
        "\"\"\")\n",
        "\n",
        "    print(\"### Strategic Recommendations for the Upcoming Year\\n\")\n",
        "    print(\"\"\"\n",
        "1.  **Investment & Expansion Strategy (Targeting Low Revenue States):**\n",
        "    * **Focus State:** Develop tailored sales programs for **NT** (Northern Territory) and **TAS** (Tasmania), the two states with the lowest sales figures.\n",
        "    * **Program Examples:** Host exclusive \"pop-up\" stores or community events in these states to boost brand visibility. Offer state-specific discounts (e.g., 'NT Locals Discount') to encourage trial.\n",
        "\n",
        "2.  **Hyper-Personalization & Next Best Offer (NBO) Programs:**\n",
        "    * **Leverage Time-of-Day Data:** The peak Evening slot suggests customers are shopping after work or during leisure time. Target this period with dedicated email/app notifications (e.g., sending NBOs at 5 PM local time).\n",
        "    * **Group-Based Targeting:** Since **Men** is the highest-selling group, develop NBOs that cross-sell high-margin Women's or Kids' apparel to them (e.g., \"Complete your family look with 20% off Women's apparel\").\n",
        "\n",
        "3.  **Inventory & Staffing Optimization:**\n",
        "    * Allocate higher inventory reserves for **Evening** shifts.\n",
        "    * Ensure appropriate staffing levels during Afternoon and Evening to handle the peak sales volume, particularly in the high-performing states like NSW and VIC.\n",
        "\"\"\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {file_name} was not found. Please ensure the file is correctly attached and available.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during processing: {e}\")\n",
        "\n",
        "# Note: The output includes simulated print statements for Markdown and requires the\n",
        "# user to run the code to see the generated charts."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "70nTe1XU3XUk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}